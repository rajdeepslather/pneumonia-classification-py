{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes to Group:\n",
    "* Normalization and transformations are from hw2 p1\n",
    "* only touched up to classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Pnuenomia Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms,models,datasets\n",
    "from sklearn.metrics import average_precision_score\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from classifier import Classifier\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms applied to the training data\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std= [0.229, 0.224, 0.225])\n",
    "\n",
    "ds_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "                    transforms.Resize(227),\n",
    "                    transforms.CenterCrop(227),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize\n",
    "                ]),\n",
    "        'test': transforms.Compose([\n",
    "                    transforms.Resize(227),\n",
    "                    transforms.CenterCrop(227),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize,\n",
    "                ]),\n",
    "        'validate': transforms.Compose([\n",
    "                    transforms.Resize(227),\n",
    "                    transforms.CenterCrop(227),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize,\n",
    "                ])\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Train, Test, and Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load our pneumonia dataset into our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dir = {\n",
    "    'train': 'data/train',\n",
    "    'test' : 'data/test',\n",
    "    'validate':  'data/val',\n",
    "}\n",
    "\n",
    "ds_set = {\n",
    "    'train': torchvision.datasets.ImageFolder(ds_dir['train'], ds_transforms['train']),\n",
    "    'test' : torchvision.datasets.ImageFolder(ds_dir['test'], ds_transforms['test']),\n",
    "    'validate':  torchvision.datasets.ImageFolder(ds_dir['validate'], ds_transforms['validate']),\n",
    "}\n",
    "# Reducing it cause there is some issue with dimentions, \n",
    "# I think the classifier gets [10, 3, 227, 227] instead of 10 * [3, 277, 277]\n",
    "# ds_batch_size = 32\n",
    "ds_batch_size = 1\n",
    "ds_loader = {\n",
    "    'train': torch.utils.data.DataLoader(ds_set['train'], batch_size=ds_batch_size,shuffle=True),\n",
    "    'test': torch.utils.data.DataLoader(ds_set['test'], batch_size=ds_batch_size,shuffle=True),\n",
    "    'validate': torch.utils.data.DataLoader(ds_set['test'], batch_size=ds_batch_size,shuffle=True),\n",
    "}\n",
    "ds_class = {ds_set['validate'].class_to_idx[i]: i for i in list(ds_set['validate'].class_to_idx.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NORMAL', 1: 'PNEUMONIA'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display 8 images\n",
    "# num_images = 8\n",
    "# TODO: Display All 8 Images\n",
    "num_images = 1\n",
    "\n",
    "ds_iter = iter(ds_loader['train'])\n",
    "images, labels = ds_iter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "#show the images, set up plt\n",
    "ds_figure = plt.figure(figsize=(num_images, 4))\n",
    "\n",
    "for i in np.arange(num_images):\n",
    "    ds_figure_subplot = ds_figure.add_subplot(2, 1, i+1, xticks=[], yticks=[]) #remove ticks and change layout\n",
    "#     ds_figure_subplot = ds_figure.add_subplot(2, num_images/2, i+1, xticks=[], yticks=[]) #remove ticks and change layout\n",
    "    image = np.transpose(images[i])\n",
    "    image = np.rot90(np.rot90(np.rot90(image)))#rotate images to be up right\n",
    "    \n",
    "    plt.imshow(image) #show image\n",
    "    ds_figure_subplot.set_title(ds_class[labels.tolist()[i]]) #add title of normal or pneumonia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_train = iter(ds_loader['train'])\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=ds_train,\n",
    "#                                                batch_size=2, \n",
    "#                                                shuffle=True,\n",
    "#                                                num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_val = iter(ds_loader['validate'])\n",
    "# val_loader = torch.utils.data.DataLoader(dataset=ds_val,\n",
    "#                                                batch_size=2, \n",
    "#                                                shuffle=True,\n",
    "#                                                num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(train_loader, classifier, criterion, optimizer):\n",
    "    classifier.train()\n",
    "    loss_ = 0.0\n",
    "    losses = []\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # TODO: Fix One Hot Encoding for Batches\n",
    "        labels = torch.nn.functional.one_hot(labels, 2)\n",
    "        images, labels = images.to(device), labels.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        logits = classifier(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss)\n",
    "    return torch.stack(losses).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(test_loader, classifier, criterion, print_ind_classes=True):\n",
    "    classifier.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        y_true = np.zeros((0,2))\n",
    "        y_score = np.zeros((0,2))\n",
    "#         y_true = np.zeros((0,21))\n",
    "#         y_score = np.zeros((0,21))\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            # TODO: Fix One Hot Encoding for Batches\n",
    "            labels = torch.nn.functional.one_hot(labels, 2)\n",
    "            images, labels = images.to(device), labels.to(device).float()\n",
    "            logits = classifier(images)\n",
    "            y_true = np.concatenate((y_true, labels.cpu().numpy()), axis=0)\n",
    "            y_score = np.concatenate((y_score, logits.cpu().numpy()), axis=0)\n",
    "            loss = criterion(logits, labels)\n",
    "            losses.append(loss)\n",
    "        aps = []\n",
    "        for i in range(y_true.shape[1]):\n",
    "            ap = average_precision_score(y_true[:, i], y_score[:, i])\n",
    "            if print_ind_classes:\n",
    "#                 print('-------  Class: {:<12}     AP: {:>8.4f}  -------'.format(VOC_CLASSES[i], ap))\n",
    "                print('-------  Class: {:<12}     AP: {:>8.4f}  -------'.format(ds_class[i], ap))\n",
    "            aps.append(ap)\n",
    "            \n",
    "        aps = np.array(aps)\n",
    "        mAP = np.mean(aps)\n",
    "        test_loss = torch.mean(torch.stack(losses))\n",
    "        print('mAP: {0:.4f}'.format(mAP))\n",
    "        print('Avg loss: {}'.format(test_loss))\n",
    "        \n",
    "    return mAP, test_loss, aps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TODO Remove This] Modifying the network \n",
    "\n",
    "The network you are given as is will allow you to reach around 0.15-0.2 mAP. To meet the benchmark for this assignment you will need to improve the network. There are a variety of different approaches you should try:\n",
    "\n",
    "* Network architecture changes\n",
    "    * Number of layers: try adding layers to make your network deeper\n",
    "    * Batch normalization: adding batch norm between layers will likely give you a significant performance increase\n",
    "    * Residual connections: as you increase the depth of your network, you will find that having residual connections like those in ResNet architectures will be helpful\n",
    "* Optimizer: Instead of plain SGD, you may want to add a learning rate schedule, add momentum, or use one of the other optimizers you have learned about like Adam. Check the `torch.optim` package for other optimizers\n",
    "* Data augmentation: You should use the `torchvision.transforms` module to try adding random resized crops and horizontal flips of the input data. Check `transforms.RandomResizedCrop` and `transforms.RandomHorizontalFlip` for this\n",
    "* Epochs: Once you have found a generally good hyperparameter setting try training for more epochs\n",
    "* Loss function: You might want to add weighting to the `MultiLabelSoftMarginLoss` for classes that are less well represented or experiment with a different loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier().to(device)\n",
    "# You can can use this function to reload a network you have already saved previously\n",
    "#classifier.load_state_dict(torch.load('voc_classifier.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use a different loss\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.SGD(classifier.parameters(), lr=0.05, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=45, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in ds_loader['train']:\n",
    "    print(torch.nn.functional.one_hot(labels, 2))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch number 1\n",
      "Loss for Training on Epoch 1 is 0.10839267820119858\n",
      "Starting epoch number 2\n",
      "Loss for Training on Epoch 2 is 0.1071133017539978\n",
      "Starting epoch number 3\n",
      "Loss for Training on Epoch 3 is 0.10779975354671478\n",
      "-------  Class: NORMAL           AP:   0.3750  -------\n",
      "-------  Class: PNEUMONIA        AP:   0.6250  -------\n",
      "mAP: 0.5000\n",
      "Avg loss: 0.12492484599351883\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 3 is 0.5\n",
      "val_loss for Testing on Epoch 3 is tensor(0.1249, device='cuda:0')\n",
      "Starting epoch number 4\n",
      "Loss for Training on Epoch 4 is 0.10740409791469574\n",
      "Starting epoch number 5\n",
      "Loss for Training on Epoch 5 is 0.10766942799091339\n",
      "Starting epoch number 6\n",
      "Loss for Training on Epoch 6 is 0.10846962779760361\n",
      "-------  Class: NORMAL           AP:   0.3750  -------\n",
      "-------  Class: PNEUMONIA        AP:   0.6250  -------\n",
      "mAP: 0.5000\n",
      "Avg loss: 0.11985185742378235\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 6 is 0.5\n",
      "val_loss for Testing on Epoch 6 is tensor(0.1199, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Training the Classifier\n",
    "# NUM_EPOCHS = 100\n",
    "NUM_EPOCHS = 6\n",
    "TEST_FREQUENCY = 3\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    print(\"Starting epoch number \" + str(epoch))\n",
    "    train_loss = train_classifier(ds_loader['train'], classifier, criterion, optimizer)\n",
    "    print(\"Loss for Training on Epoch \" +str(epoch) + \" is \"+ str(train_loss))\n",
    "    scheduler.step()\n",
    "    if(epoch%TEST_FREQUENCY==0):\n",
    "        mAP_val, val_loss, _ = test_classifier(ds_loader['validate'], classifier, criterion)\n",
    "        print('Evaluating classifier')\n",
    "        print(\"Mean Precision Score for Testing on Epoch \" +str(epoch) + \" is \"+ str(mAP_val))\n",
    "        print(\"val_loss for Testing on Epoch \" +str(epoch) + \" is \"+ str(val_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the clssifier network\n",
    "# Suggestion: you can save checkpoints of your network during training and reload them later\n",
    "torch.save(classifier.state_dict(), './voc_classifier.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------  Class: NORMAL           AP:   0.3750  -------\n",
      "-------  Class: PNEUMONIA        AP:   0.6250  -------\n",
      "mAP: 0.5000\n",
      "Avg loss: 0.11985187232494354\n"
     ]
    }
   ],
   "source": [
    "# ds_test = VocDataset('VOCdevkit_2007/VOC2007test/','test', test_transform)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=ds_test,\n",
    "#                                                batch_size=50, \n",
    "#                                                shuffle=False,\n",
    "#                                                num_workers=1)\n",
    "\n",
    "mAP_test, test_loss, test_aps = test_classifier(ds_loader['test'], classifier, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_submission_csv('my_solution.csv', test_aps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
