{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes to Group:\n",
    "* Normalization and transformations are from hw2 p1\n",
    "* only touched up to classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Pnuenomia Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms,models,datasets\n",
    "from sklearn.metrics import average_precision_score\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from classifier import Classifier\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms applied to the training data\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std= [0.229, 0.224, 0.225])\n",
    "\n",
    "ds_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "                    transforms.Resize(227),\n",
    "                    transforms.CenterCrop(227),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize\n",
    "                ]),\n",
    "        'test': transforms.Compose([\n",
    "                    transforms.Resize(227),\n",
    "                    transforms.CenterCrop(227),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize,\n",
    "                ]),\n",
    "        'validate': transforms.Compose([\n",
    "                    transforms.Resize(227),\n",
    "                    transforms.CenterCrop(227),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize,\n",
    "                ])\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Train, Test, and Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load our pneumonia dataset into our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dir = {\n",
    "    'train': 'data/train',\n",
    "    'test' : 'data/test',\n",
    "    'validate':  'data/val',\n",
    "}\n",
    "\n",
    "ds_set = {\n",
    "    'train': torchvision.datasets.ImageFolder(ds_dir['train'], ds_transforms['train']),\n",
    "    'test' : torchvision.datasets.ImageFolder(ds_dir['test'], ds_transforms['test']),\n",
    "    'validate':  torchvision.datasets.ImageFolder(ds_dir['validate'], ds_transforms['validate']),\n",
    "}\n",
    "# Reducing it cause there is some issue with dimentions, \n",
    "# I think the classifier gets [10, 3, 227, 227] instead of 10 * [3, 277, 277]\n",
    "# ds_batch_size = 32\n",
    "ds_batch_size = 1\n",
    "ds_loader = {\n",
    "    'train': torch.utils.data.DataLoader(ds_set['train'], batch_size=ds_batch_size,shuffle=True),\n",
    "    'test': torch.utils.data.DataLoader(ds_set['test'], batch_size=ds_batch_size,shuffle=True),\n",
    "    'validate': torch.utils.data.DataLoader(ds_set['test'], batch_size=ds_batch_size,shuffle=True),\n",
    "}\n",
    "ds_class = {ds_set['validate'].class_to_idx[i]: i for i in list(ds_set['validate'].class_to_idx.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NORMAL', 1: 'PNEUMONIA'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display 8 images\n",
    "# num_images = 8\n",
    "# TODO: Display All 8 Images\n",
    "num_images = 1\n",
    "\n",
    "ds_iter = iter(ds_loader['train'])\n",
    "images, labels = ds_iter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "#show the images, set up plt\n",
    "ds_figure = plt.figure(figsize=(num_images, 4))\n",
    "\n",
    "for i in np.arange(num_images):\n",
    "    ds_figure_subplot = ds_figure.add_subplot(2, 1, i+1, xticks=[], yticks=[]) #remove ticks and change layout\n",
    "#     ds_figure_subplot = ds_figure.add_subplot(2, num_images/2, i+1, xticks=[], yticks=[]) #remove ticks and change layout\n",
    "    image = np.transpose(images[i])\n",
    "    image = np.rot90(np.rot90(np.rot90(image)))#rotate images to be up right\n",
    "    \n",
    "    plt.imshow(image) #show image\n",
    "    ds_figure_subplot.set_title(ds_class[labels.tolist()[i]]) #add title of normal or pneumonia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_train = iter(ds_loader['train'])\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=ds_train,\n",
    "#                                                batch_size=2, \n",
    "#                                                shuffle=True,\n",
    "#                                                num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_val = iter(ds_loader['validate'])\n",
    "# val_loader = torch.utils.data.DataLoader(dataset=ds_val,\n",
    "#                                                batch_size=2, \n",
    "#                                                shuffle=True,\n",
    "#                                                num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(train_loader, classifier, criterion, optimizer):\n",
    "    classifier.train()\n",
    "    loss_ = 0.0\n",
    "    losses = []\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # TODO: Fix One Hot Encoding for Batches\n",
    "        labels = torch.nn.functional.one_hot(labels, 2)\n",
    "        images, labels = images.to(device), labels.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        logits = classifier(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss)\n",
    "    return torch.stack(losses).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(test_loader, classifier, criterion, print_ind_classes=True):\n",
    "    classifier.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        y_true = np.zeros((0,2))\n",
    "        y_score = np.zeros((0,2))\n",
    "#         y_true = np.zeros((0,21))\n",
    "#         y_score = np.zeros((0,21))\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            # TODO: Fix One Hot Encoding for Batches\n",
    "            labels = torch.nn.functional.one_hot(labels, 2)\n",
    "            images, labels = images.to(device), labels.to(device).float()\n",
    "            logits = classifier(images)\n",
    "            y_true = np.concatenate((y_true, labels.cpu().numpy()), axis=0)\n",
    "            y_score = np.concatenate((y_score, logits.cpu().numpy()), axis=0)\n",
    "            loss = criterion(logits, labels)\n",
    "            losses.append(loss)\n",
    "        aps = []\n",
    "        for i in range(y_true.shape[1]):\n",
    "            ap = average_precision_score(y_true[:, i], y_score[:, i])\n",
    "            if print_ind_classes:\n",
    "#                 print('-------  Class: {:<12}     AP: {:>8.4f}  -------'.format(VOC_CLASSES[i], ap))\n",
    "                print('-------  Class: {:<12}     AP: {:>8.4f}  -------'.format(ds_class[i], ap))\n",
    "            aps.append(ap)\n",
    "            \n",
    "        aps = np.array(aps)\n",
    "        mAP = np.mean(aps)\n",
    "        test_loss = torch.mean(torch.stack(losses))\n",
    "        print('mAP: {0:.4f}'.format(mAP))\n",
    "        print('Avg loss: {}'.format(test_loss))\n",
    "        \n",
    "    return mAP, test_loss, aps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TODO Remove This] Modifying the network \n",
    "\n",
    "The network you are given as is will allow you to reach around 0.15-0.2 mAP. To meet the benchmark for this assignment you will need to improve the network. There are a variety of different approaches you should try:\n",
    "\n",
    "* Network architecture changes\n",
    "    * Number of layers: try adding layers to make your network deeper\n",
    "    * Batch normalization: adding batch norm between layers will likely give you a significant performance increase\n",
    "    * Residual connections: as you increase the depth of your network, you will find that having residual connections like those in ResNet architectures will be helpful\n",
    "* Optimizer: Instead of plain SGD, you may want to add a learning rate schedule, add momentum, or use one of the other optimizers you have learned about like Adam. Check the `torch.optim` package for other optimizers\n",
    "* Data augmentation: You should use the `torchvision.transforms` module to try adding random resized crops and horizontal flips of the input data. Check `transforms.RandomResizedCrop` and `transforms.RandomHorizontalFlip` for this\n",
    "* Epochs: Once you have found a generally good hyperparameter setting try training for more epochs\n",
    "* Loss function: You might want to add weighting to the `MultiLabelSoftMarginLoss` for classes that are less well represented or experiment with a different loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier().to(device)\n",
    "# You can can use this function to reload a network you have already saved previously\n",
    "#classifier.load_state_dict(torch.load('voc_classifier.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use a different loss\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.SGD(classifier.parameters(), lr=0.05, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=45, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in ds_loader['train']:\n",
    "    print(torch.nn.functional.one_hot(labels, 2))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch number 1\n",
      "Loss for Training on Epoch 1 is 0.10825250297784805\n",
      "Starting epoch number 2\n",
      "Loss for Training on Epoch 2 is 0.10673418641090393\n",
      "Starting epoch number 3\n",
      "Loss for Training on Epoch 3 is 0.10819513350725174\n",
      "-------  Class: NORMAL           AP:   0.3750  -------\n",
      "-------  Class: PNEUMONIA        AP:   0.6250  -------\n",
      "mAP: 0.5000\n",
      "Avg loss: 0.12145280838012695\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 3 is 0.5\n",
      "val_loss for Testing on Epoch 3 is tensor(0.1215, device='cuda:0')\n",
      "Starting epoch number 4\n",
      "Loss for Training on Epoch 4 is 0.10770543664693832\n",
      "Starting epoch number 5\n",
      "Loss for Training on Epoch 5 is 0.1084601953625679\n",
      "Starting epoch number 6\n",
      "Loss for Training on Epoch 6 is 0.10751761496067047\n",
      "-------  Class: NORMAL           AP:   0.3750  -------\n",
      "-------  Class: PNEUMONIA        AP:   0.6250  -------\n",
      "mAP: 0.5000\n",
      "Avg loss: 0.11726999282836914\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 6 is 0.5\n",
      "val_loss for Testing on Epoch 6 is tensor(0.1173, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Training the Classifier\n",
    "# NUM_EPOCHS = 100\n",
    "NUM_EPOCHS = 6\n",
    "TEST_FREQUENCY = 3\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    print(\"Starting epoch number \" + str(epoch))\n",
    "    train_loss = train_classifier(ds_loader['train'], classifier, criterion, optimizer)\n",
    "    print(\"Loss for Training on Epoch \" +str(epoch) + \" is \"+ str(train_loss))\n",
    "    scheduler.step()\n",
    "    if(epoch%TEST_FREQUENCY==0):\n",
    "        mAP_val, val_loss, _ = test_classifier(ds_loader['validate'], classifier, criterion)\n",
    "        print('Evaluating classifier')\n",
    "        print(\"Mean Precision Score for Testing on Epoch \" +str(epoch) + \" is \"+ str(mAP_val))\n",
    "        print(\"val_loss for Testing on Epoch \" +str(epoch) + \" is \"+ str(val_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the clssifier network\n",
    "# Suggestion: you can save checkpoints of your network during training and reload them later\n",
    "torch.save(classifier.state_dict(), './voc_classifier.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------  Class: NORMAL           AP:   0.3750  -------\n",
      "-------  Class: PNEUMONIA        AP:   0.6250  -------\n",
      "mAP: 0.5000\n",
      "Avg loss: 0.11726998537778854\n"
     ]
    }
   ],
   "source": [
    "# ds_test = VocDataset('VOCdevkit_2007/VOC2007test/','test', test_transform)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=ds_test,\n",
    "#                                                batch_size=50, \n",
    "#                                                shuffle=False,\n",
    "#                                                num_workers=1)\n",
    "\n",
    "mAP_test, test_loss, test_aps = test_classifier(ds_loader['test'], classifier, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_submission_csv('my_solution.csv', test_aps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
